{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "path = os.path.join(os.path.dirname(os.path.abspath(\"__file__\")), 'keras_yolo3/')\n",
    "sys.path.append(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Class definition of YOLO_v3 style detection model on image and video\n",
    "\"\"\"\n",
    "import colorsys\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "\n",
    "from yolo3.model import yolo_eval, yolo_body, tiny_yolo_body\n",
    "from yolo3.utils import letterbox_image\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyYOLO(object):\n",
    "    yolo_path = \"keras_yolo3/\"\n",
    "    _defaults = {\n",
    "        \"model_path\": yolo_path + 'model_data/yolo.h5',\n",
    "        \"anchors_path\": yolo_path + 'model_data/yolo_anchors.txt',\n",
    "        \"classes_path\": yolo_path + 'model_data/coco_classes.txt',\n",
    "        \"score\" : 0.3,\n",
    "        \"iou\" : 0.45,\n",
    "        \"model_image_size\" : (416, 416),\n",
    "        \"gpu_num\" : 1,\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_defaults(cls, n):\n",
    "        if n in cls._defaults:\n",
    "            return cls._defaults[n]\n",
    "        else:\n",
    "            return \"Unrecognized attribute name '\" + n + \"'\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(self._defaults) # set up default values\n",
    "        self.__dict__.update(kwargs) # and update with user overrides\n",
    "        self.class_names = self._get_class()\n",
    "        self.anchors = self._get_anchors()\n",
    "        self.sess = K.get_session()\n",
    "        self.boxes, self.scores, self.classes = self.generate()\n",
    "\n",
    "    def _get_class(self):\n",
    "        classes_path = os.path.expanduser(self.classes_path)\n",
    "        with open(classes_path) as f:\n",
    "            class_names = f.readlines()\n",
    "        class_names = [c.strip() for c in class_names]\n",
    "        return class_names\n",
    "\n",
    "    def _get_anchors(self):\n",
    "        anchors_path = os.path.expanduser(self.anchors_path)\n",
    "        with open(anchors_path) as f:\n",
    "            anchors = f.readline()\n",
    "        anchors = [float(x) for x in anchors.split(',')]\n",
    "        return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "    def generate(self):\n",
    "        model_path = os.path.expanduser(self.model_path)\n",
    "        assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'\n",
    "\n",
    "        # Load model, or construct model and load weights.\n",
    "        num_anchors = len(self.anchors)\n",
    "        num_classes = len(self.class_names)\n",
    "        is_tiny_version = num_anchors==6 # default setting\n",
    "        try:\n",
    "            self.yolo_model = load_model(model_path, compile=False)\n",
    "        except:\n",
    "            self.yolo_model = tiny_yolo_body(Input(shape=(None,None,3)), num_anchors//2, num_classes) \\\n",
    "                if is_tiny_version else yolo_body(Input(shape=(None,None,3)), num_anchors//3, num_classes)\n",
    "            self.yolo_model.load_weights(self.model_path) # make sure model, anchors and classes match\n",
    "        else:\n",
    "            assert self.yolo_model.layers[-1].output_shape[-1] == \\\n",
    "                num_anchors/len(self.yolo_model.output) * (num_classes + 5), \\\n",
    "                'Mismatch between model and given anchor and class sizes'\n",
    "\n",
    "        print('{} model, anchors, and classes loaded.'.format(model_path))\n",
    "\n",
    "        # Generate colors for drawing bounding boxes.\n",
    "        hsv_tuples = [(x / len(self.class_names), 1., 1.)\n",
    "                      for x in range(len(self.class_names))]\n",
    "        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "        self.colors = list(\n",
    "            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\n",
    "                self.colors))\n",
    "        np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
    "        np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.\n",
    "        np.random.seed(None)  # Reset seed to default.\n",
    "\n",
    "        # Generate output tensor targets for filtered bounding boxes.\n",
    "        self.input_image_shape = K.placeholder(shape=(2, ))\n",
    "        if self.gpu_num>=2:\n",
    "            self.yolo_model = multi_gpu_model(self.yolo_model, gpus=self.gpu_num)\n",
    "        boxes, scores, classes = yolo_eval(self.yolo_model.output, self.anchors,\n",
    "                len(self.class_names), self.input_image_shape,\n",
    "                score_threshold=self.score, iou_threshold=self.iou)\n",
    "        return boxes, scores, classes\n",
    "\n",
    "    def detect_image(self, image):\n",
    "        start = timer()\n",
    "        if self.model_image_size != (None, None):\n",
    "            assert self.model_image_size[0]%32 == 0, 'Multiples of 32 required'\n",
    "            assert self.model_image_size[1]%32 == 0, 'Multiples of 32 required'\n",
    "            print(image)\n",
    "            \n",
    "            boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size)))\n",
    "\n",
    "        else:\n",
    "            new_image_size = (image.width - (image.width % 32),\n",
    "                              image.height - (image.height % 32))\n",
    "            boxed_image = letterbox_image(image, new_image_size)\n",
    "        image_data = np.array(boxed_image, dtype='float32')\n",
    "\n",
    "        print(image_data.shape)\n",
    "        image_data /= 255.\n",
    "        image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
    "\n",
    "        out_boxes, out_scores, out_classes = self.sess.run(\n",
    "            [self.boxes, self.scores, self.classes],\n",
    "            feed_dict={\n",
    "                self.yolo_model.input: image_data,\n",
    "                self.input_image_shape: [image.size[1], image.size[0]],\n",
    "                K.learning_phase(): 0\n",
    "            })\n",
    "\n",
    "        print('Found {} boxes for {}'.format(len(out_boxes), 'img'))\n",
    "\n",
    "        font = ImageFont.truetype(font= self.yolo_path + 'font/FiraMono-Medium.otf',\n",
    "                    size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
    "        thickness = (image.size[0] + image.size[1]) // 300\n",
    "\n",
    "        for i, c in reversed(list(enumerate(out_classes))):\n",
    "            predicted_class = self.class_names[c]\n",
    "            box = out_boxes[i]\n",
    "            score = out_scores[i]\n",
    "\n",
    "            label = '{} {:.2f}'.format(predicted_class, score)\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            label_size = draw.textsize(label, font)\n",
    "\n",
    "            top, left, bottom, right = box\n",
    "            top = max(0, np.floor(top + 0.5).astype('int32'))\n",
    "            left = max(0, np.floor(left + 0.5).astype('int32'))\n",
    "            bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
    "            right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
    "            print(label, (left, top), (right, bottom))\n",
    "\n",
    "            if top - label_size[1] >= 0:\n",
    "                text_origin = np.array([left, top - label_size[1]])\n",
    "            else:\n",
    "                text_origin = np.array([left, top + 1])\n",
    "\n",
    "            # My kingdom for a good redistributable image drawing library.\n",
    "            for i in range(thickness):\n",
    "                draw.rectangle(\n",
    "                    [left + i, top + i, right - i, bottom - i],\n",
    "                    outline=self.colors[c])\n",
    "            draw.rectangle(\n",
    "                [tuple(text_origin), tuple(text_origin + label_size)],\n",
    "                fill=self.colors[c])\n",
    "            draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
    "            del draw\n",
    "\n",
    "        end = timer()\n",
    "        print(end - start)\n",
    "        return image, out_boxes, out_scores, out_classes\n",
    "\n",
    "    def close_session(self):\n",
    "        self.sess.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/naka345/work/venv/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "keras_yolo3/model_data/yolo.h5 model, anchors, and classes loaded.\n"
     ]
    }
   ],
   "source": [
    "myYolo= MyYOLO()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_4474.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129AFD208>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 0.77 (14, 0) (738, 417)\n",
      "car 0.99 (824, 371) (3007, 2833)\n",
      "1.5275399670026673\n",
      "lu_x     720\n",
      "lu_y    1585\n",
      "ru_x    1323\n",
      "ru_y    1806\n",
      "ld_x     672\n",
      "ld_y    1825\n",
      "rd_x    1242\n",
      "rd_y    2050\n",
      "Name: 4474, dtype: object\n",
      "IMG_4449.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D2C7F0>\n",
      "(416, 416, 3)\n",
      "Found 4 boxes for img\n",
      "truck 0.37 (2559, 437) (3011, 1264)\n",
      "car 0.67 (2559, 437) (3011, 1264)\n",
      "car 1.00 (20, 231) (1309, 1260)\n",
      "car 1.00 (379, 260) (2781, 1985)\n",
      "1.5639528079991578\n",
      "lu_x     337\n",
      "lu_y    1044\n",
      "ru_x     643\n",
      "ru_y    1134\n",
      "ld_x     331\n",
      "ld_y    1221\n",
      "rd_x     628\n",
      "rd_y    1317\n",
      "Name: 4449, dtype: object\n",
      "IMG_4462.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D2C7F0>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 0.97 (8, 15) (1250, 1018)\n",
      "car 0.97 (917, 16) (3024, 2045)\n",
      "1.5634035679977387\n",
      "lu_x     400\n",
      "lu_y    1226\n",
      "ru_x     778\n",
      "ru_y    1361\n",
      "ld_x     382\n",
      "ld_y    1436\n",
      "rd_x     748\n",
      "rd_y    1576\n",
      "Name: 4462, dtype: object\n",
      "IMG_4516.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129AD4080>\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "car 0.96 (739, 441) (2654, 1486)\n",
      "1.5953088640017086\n",
      "lu_x    342\n",
      "lu_y    492\n",
      "ru_x    574\n",
      "ru_y    534\n",
      "ld_x    312\n",
      "ld_y    618\n",
      "rd_x    546\n",
      "rd_y    669\n",
      "Name: 4516, dtype: object\n",
      "IMG_4513.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D2B8D0>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "truck 0.97 (997, 185) (2730, 1485)\n",
      "car 1.00 (8, 314) (1313, 961)\n",
      "1.5853836790010973\n",
      "lu_x     85\n",
      "lu_y    747\n",
      "ru_x    322\n",
      "ru_y    795\n",
      "ld_x     56\n",
      "ld_y    876\n",
      "rd_x    289\n",
      "rd_y    928\n",
      "Name: 4513, dtype: object\n",
      "IMG_4507.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129AFD160>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 0.80 (2437, 245) (3022, 1831)\n",
      "car 0.96 (739, 112) (2500, 1377)\n",
      "1.5256157759977214\n",
      "lu_x    308\n",
      "lu_y    682\n",
      "ru_x    531\n",
      "ru_y    739\n",
      "ld_x    293\n",
      "ld_y    804\n",
      "rd_x    516\n",
      "rd_y    861\n",
      "Name: 4507, dtype: object\n",
      "IMG_4459.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129B56EB8>\n",
      "(416, 416, 3)\n",
      "Found 4 boxes for img\n",
      "diningtable 0.38 (118, 349) (2968, 3718)\n",
      "car 0.83 (729, 264) (3001, 2447)\n",
      "car 0.99 (673, 224) (1517, 769)\n",
      "car 1.00 (8, 19) (709, 857)\n",
      "1.5494940250027867\n",
      "IMG_4471.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D2BCC0>\n",
      "(416, 416, 3)\n",
      "Found 3 boxes for img\n",
      "car 0.31 (13, 48) (616, 460)\n",
      "car 1.00 (991, 393) (3024, 2780)\n",
      "car 1.00 (0, 223) (2031, 1705)\n",
      "1.5463635220003198\n",
      "lu_x     538\n",
      "lu_y    1376\n",
      "ru_x     997\n",
      "ru_y    1565\n",
      "ld_x     491\n",
      "ld_y    1598\n",
      "rd_x     931\n",
      "rd_y    1794\n",
      "Name: 4471, dtype: object\n",
      "IMG_4510.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x125E61278>\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "car 0.97 (871, 198) (2593, 1455)\n",
      "1.526410341997689\n",
      "lu_x    376\n",
      "lu_y    737\n",
      "ru_x    628\n",
      "ru_y    801\n",
      "ld_x    359\n",
      "ld_y    869\n",
      "rd_x    602\n",
      "rd_y    935\n",
      "Name: 4510, dtype: object\n",
      "IMG_4439.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x12A3042E8>\n",
      "(416, 416, 3)\n",
      "Found 7 boxes for img\n",
      "truck 0.32 (1103, 559) (1760, 881)\n",
      "truck 0.69 (5, 23) (734, 1156)\n",
      "car 0.54 (960, 559) (1243, 710)\n",
      "car 0.59 (20, 72) (732, 1208)\n",
      "car 0.79 (1103, 559) (1760, 881)\n",
      "car 0.97 (657, 530) (1256, 973)\n",
      "car 0.99 (477, 576) (3024, 2673)\n",
      "1.558546729000227\n",
      "lu_x     607\n",
      "lu_y    1406\n",
      "ru_x    1067\n",
      "ru_y    1552\n",
      "ld_x     581\n",
      "ld_y    1630\n",
      "rd_x    1039\n",
      "rd_y    1790\n",
      "Name: 4439, dtype: object\n",
      "IMG_4438.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129AFD320>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 1.00 (42, 272) (1674, 1501)\n",
      "car 1.00 (713, 378) (2932, 2302)\n",
      "1.5377807580007357\n",
      "lu_x     457\n",
      "lu_y    1222\n",
      "ru_x     829\n",
      "ru_y    1342\n",
      "ld_x     431\n",
      "ld_y    1412\n",
      "rd_x     789\n",
      "rd_y    1536\n",
      "Name: 4438, dtype: object\n",
      "IMG_4422.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129AD4080>\n",
      "(416, 416, 3)\n",
      "Found 4 boxes for img\n",
      "truck 0.75 (998, 405) (2840, 2117)\n",
      "car 0.79 (968, 332) (2864, 1980)\n",
      "car 0.96 (2743, 852) (3024, 1409)\n",
      "car 0.99 (0, 494) (1311, 1559)\n",
      "1.5446614819993556\n",
      "IMG_4436.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D42DD8>\n",
      "(416, 416, 3)\n",
      "Found 4 boxes for img\n",
      "car 0.64 (884, 210) (1353, 620)\n",
      "car 0.77 (0, 121) (421, 552)\n",
      "car 0.85 (186, 112) (860, 579)\n",
      "car 0.96 (543, 62) (3024, 2759)\n",
      "1.571114430000307\n",
      "lu_x     691\n",
      "lu_y    1704\n",
      "ru_x    1163\n",
      "ru_y    1860\n",
      "ld_x     663\n",
      "ld_y    1922\n",
      "rd_x    1119\n",
      "rd_y    2084\n",
      "Name: 4436, dtype: object\n",
      "IMG_4437.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x125E61278>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 0.99 (802, 254) (3024, 2096)\n",
      "car 1.00 (0, 85) (1977, 1283)\n",
      "1.5273601440021594\n",
      "lu_x     294\n",
      "lu_y    1178\n",
      "ru_x     604\n",
      "ru_y    1302\n",
      "ld_x     276\n",
      "ld_y    1354\n",
      "rd_x     570\n",
      "rd_y    1480\n",
      "Name: 4437, dtype: object\n",
      "IMG_4423.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x125E6E128>\n",
      "(416, 416, 3)\n",
      "Found 3 boxes for img\n",
      "car 0.88 (4, 355) (474, 668)\n",
      "car 0.99 (367, 494) (3024, 2096)\n",
      "car 0.99 (0, 372) (1671, 1339)\n",
      "1.5271825099989655\n",
      "lu_x     325\n",
      "lu_y    1027\n",
      "ru_x     586\n",
      "ru_y    1125\n",
      "ld_x     319\n",
      "ld_y    1192\n",
      "rd_x     578\n",
      "rd_y    1296\n",
      "Name: 4423, dtype: object\n",
      "IMG_4435.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129B56EB8>\n",
      "(416, 416, 3)\n",
      "Found 3 boxes for img\n",
      "diningtable 0.32 (55, 204) (2944, 3806)\n",
      "car 0.99 (799, 0) (3022, 2980)\n",
      "car 1.00 (0, 24) (2295, 1451)\n",
      "1.5606595629978983\n",
      "IMG_4421.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129B56EB8>\n",
      "(416, 416, 3)\n",
      "Found 6 boxes for img\n",
      "truck 0.35 (110, 561) (1550, 1578)\n",
      "truck 0.58 (1129, 664) (2790, 1857)\n",
      "car 0.77 (1144, 595) (2864, 1836)\n",
      "car 0.87 (110, 561) (1550, 1578)\n",
      "car 0.93 (2691, 888) (2942, 1368)\n",
      "car 0.97 (0, 724) (385, 1355)\n",
      "1.5704967799974838\n",
      "IMG_4430.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D2B8D0>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 1.00 (39, 482) (1880, 1638)\n",
      "car 1.00 (792, 597) (3005, 2360)\n",
      "1.5400080459985475\n",
      "lu_x     394\n",
      "lu_y    1061\n",
      "ru_x     686\n",
      "ru_y    1163\n",
      "ld_x     382\n",
      "ld_y    1217\n",
      "rd_x     664\n",
      "rd_y    1323\n",
      "Name: 4430, dtype: object\n",
      "IMG_4424.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129AFD208>\n",
      "(416, 416, 3)\n",
      "Found 3 boxes for img\n",
      "car 0.70 (12, 364) (444, 730)\n",
      "car 0.99 (975, 608) (3012, 2111)\n",
      "car 1.00 (6, 450) (1780, 1627)\n",
      "1.5461985930014635\n",
      "lu_x     217\n",
      "lu_y     970\n",
      "ru_x     477\n",
      "ru_y    1044\n",
      "ld_x     205\n",
      "ld_y    1118\n",
      "rd_x     457\n",
      "rd_y    1200\n",
      "Name: 4424, dtype: object\n",
      "IMG_4431.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D2BCC0>\n",
      "(416, 416, 3)\n",
      "Found 6 boxes for img\n",
      "car 0.31 (1644, 662) (1884, 857)\n",
      "car 0.60 (1746, 571) (2639, 888)\n",
      "car 0.93 (1319, 558) (1863, 891)\n",
      "car 0.95 (11, 271) (1026, 1107)\n",
      "car 0.99 (712, 649) (3024, 2533)\n",
      "car 1.00 (981, 563) (1589, 1015)\n",
      "1.5626071469996532\n",
      "lu_x     335\n",
      "lu_y    1022\n",
      "ru_x     635\n",
      "ru_y    1151\n",
      "ld_x     314\n",
      "ld_y    1217\n",
      "rd_x     620\n",
      "rd_y    1346\n",
      "Name: 4431, dtype: object\n",
      "IMG_4427.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D42550>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 0.98 (0, 199) (1842, 1529)\n",
      "car 1.00 (879, 599) (2991, 2167)\n",
      "1.5279468199987605\n",
      "lu_x     417\n",
      "lu_y     915\n",
      "ru_x     713\n",
      "ru_y    1009\n",
      "ld_x     391\n",
      "ld_y    1073\n",
      "rd_x     681\n",
      "rd_y    1169\n",
      "Name: 4427, dtype: object\n",
      "IMG_4433.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129AFD748>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 1.00 (4, 128) (1912, 1387)\n",
      "car 1.00 (774, 151) (3024, 2531)\n",
      "1.555439463998482\n",
      "lu_x     520\n",
      "lu_y    1529\n",
      "ru_x     910\n",
      "ru_y    1693\n",
      "ld_x     500\n",
      "ld_y    1713\n",
      "rd_x     870\n",
      "rd_y    1889\n",
      "Name: 4433, dtype: object\n",
      "IMG_4432.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129AA5710>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 1.00 (18, 158) (1782, 1534)\n",
      "car 1.00 (703, 293) (3024, 2108)\n",
      "1.5440387599992391\n",
      "lu_x     413\n",
      "lu_y    1135\n",
      "ru_x     747\n",
      "ru_y    1237\n",
      "ld_x     389\n",
      "ld_y    1325\n",
      "rd_x     715\n",
      "rd_y    1431\n",
      "Name: 4432, dtype: object\n",
      "IMG_4509.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D42F98>\n",
      "(416, 416, 3)\n",
      "Found 4 boxes for img\n",
      "truck 0.40 (1050, 0) (3020, 1342)\n",
      "car 0.64 (2682, 908) (3011, 1802)\n",
      "car 0.87 (1036, 0) (2785, 1474)\n",
      "car 0.99 (29, 205) (1199, 1087)\n",
      "1.5550950649994775\n",
      "IMG_4496.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D42DD8>\n",
      "(416, 416, 3)\n",
      "Found 4 boxes for img\n",
      "car 0.37 (53, 66) (1105, 703)\n",
      "car 0.84 (1183, 285) (2831, 1266)\n",
      "car 0.90 (284, 172) (2030, 1136)\n",
      "car 0.98 (2628, 808) (3009, 1522)\n",
      "1.5628200690007361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMG_4441.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129AA5710>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 0.98 (556, 0) (2999, 1842)\n",
      "car 1.00 (0, 94) (1358, 1190)\n",
      "1.5248671859990282\n",
      "lu_x     436\n",
      "lu_y    1118\n",
      "ru_x     724\n",
      "ru_y    1214\n",
      "ld_x     416\n",
      "ld_y    1290\n",
      "rd_x     700\n",
      "rd_y    1392\n",
      "Name: 4441, dtype: object\n",
      "IMG_4469.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D2BCC0>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 0.94 (0, 0) (2447, 1579)\n",
      "car 0.96 (825, 87) (3003, 2556)\n",
      "1.5349999120007851\n",
      "lu_x     601\n",
      "lu_y    1497\n",
      "ru_x    1079\n",
      "ru_y    1673\n",
      "ld_x     571\n",
      "ld_y    1733\n",
      "rd_x    1031\n",
      "rd_y    1903\n",
      "Name: 4469, dtype: object\n",
      "IMG_4468.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x12A3042E8>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 0.81 (141, 18) (2254, 1619)\n",
      "car 0.99 (1295, 351) (2976, 2637)\n",
      "1.5317631229991093\n",
      "lu_x     640\n",
      "lu_y    1544\n",
      "ru_x    1152\n",
      "ru_y    1722\n",
      "ld_x     582\n",
      "ld_y    1757\n",
      "rd_x    1063\n",
      "rd_y    1950\n",
      "Name: 4468, dtype: object\n",
      "IMG_4508.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D2B8D0>\n",
      "(416, 416, 3)\n",
      "Found 3 boxes for img\n",
      "truck 0.77 (10, 57) (1784, 982)\n",
      "car 0.41 (1, 34) (1313, 990)\n",
      "car 0.97 (762, 295) (2435, 1330)\n",
      "1.543114121999679\n",
      "lu_x    212\n",
      "lu_y    671\n",
      "ru_x    399\n",
      "ru_y    713\n",
      "ld_x    213\n",
      "ld_y    779\n",
      "rd_x    393\n",
      "rd_y    827\n",
      "Name: 4508, dtype: object\n",
      "IMG_4495.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129AA5710>\n",
      "(416, 416, 3)\n",
      "Found 4 boxes for img\n",
      "truck 0.42 (0, 193) (1580, 1091)\n",
      "car 0.82 (0, 111) (1570, 1079)\n",
      "car 0.99 (2670, 804) (3020, 1590)\n",
      "car 1.00 (1154, 433) (2737, 1461)\n",
      "1.5513278810030897\n",
      "lu_x    238\n",
      "lu_y    587\n",
      "ru_x    415\n",
      "ru_y    625\n",
      "ld_x    231\n",
      "ld_y    691\n",
      "rd_x    402\n",
      "rd_y    727\n",
      "Name: 4495, dtype: object\n",
      "IMG_4442.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D2BCC0>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 1.00 (0, 253) (1908, 1277)\n",
      "car 1.00 (792, 494) (2982, 2131)\n",
      "1.556905055000243\n",
      "lu_x     440\n",
      "lu_y     973\n",
      "ru_x     761\n",
      "ru_y    1086\n",
      "ld_x     416\n",
      "ld_y    1147\n",
      "rd_x     728\n",
      "rd_y    1264\n",
      "Name: 4442, dtype: object\n",
      "IMG_4443.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D2B8D0>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 0.99 (4, 17) (1669, 1316)\n",
      "car 1.00 (648, 299) (2946, 1916)\n",
      "1.5226991449999332\n",
      "lu_x     272\n",
      "lu_y     987\n",
      "ru_x     554\n",
      "ru_y    1086\n",
      "ld_x     249\n",
      "ld_y    1150\n",
      "rd_x     528\n",
      "rd_y    1249\n",
      "Name: 4443, dtype: object\n",
      "IMG_4490.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129AFD320>\n",
      "(416, 416, 3)\n",
      "Found 4 boxes for img\n",
      "car 0.41 (2138, 419) (2654, 879)\n",
      "car 0.88 (2378, 431) (3017, 1279)\n",
      "car 0.99 (871, 250) (2048, 1086)\n",
      "car 1.00 (17, 72) (1202, 847)\n",
      "1.5720921370011638\n",
      "lu_x    266\n",
      "lu_y    534\n",
      "ru_x    436\n",
      "ru_y    564\n",
      "ld_x    258\n",
      "ld_y    624\n",
      "rd_x    427\n",
      "rd_y    658\n",
      "Name: 4490, dtype: object\n",
      "IMG_4453.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129B56EB8>\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "car 1.00 (586, 0) (3024, 1667)\n",
      "1.5719177480023063\n",
      "lu_x     329\n",
      "lu_y     879\n",
      "ru_x     583\n",
      "ru_y     965\n",
      "ld_x     313\n",
      "ld_y    1052\n",
      "rd_x     553\n",
      "rd_y    1142\n",
      "Name: 4453, dtype: object\n",
      "IMG_4447.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D42F98>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 0.98 (0, 0) (1757, 1185)\n",
      "car 0.99 (592, 15) (3007, 2096)\n",
      "1.5378610959996877\n",
      "lu_x     634\n",
      "lu_y    1263\n",
      "ru_x     976\n",
      "ru_y    1385\n",
      "ld_x     604\n",
      "ld_y    1445\n",
      "rd_x     942\n",
      "rd_y    1575\n",
      "Name: 4447, dtype: object\n",
      "IMG_4452.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D425F8>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 0.52 (434, 73) (1916, 662)\n",
      "car 1.00 (899, 275) (2908, 1639)\n",
      "1.6271079150028527\n",
      "lu_x    136\n",
      "lu_y    781\n",
      "ru_x    364\n",
      "ru_y    849\n",
      "ld_x    132\n",
      "ld_y    925\n",
      "rd_x    358\n",
      "rd_y    996\n",
      "Name: 4452, dtype: object\n",
      "IMG_4444.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129AA5710>\n",
      "(416, 416, 3)\n",
      "Found 3 boxes for img\n",
      "car 0.98 (811, 255) (3024, 2314)\n",
      "car 0.98 (0, 160) (737, 1113)\n",
      "car 0.99 (627, 411) (1404, 924)\n",
      "1.598463955000625\n",
      "lu_x     515\n",
      "lu_y    1307\n",
      "ru_x     910\n",
      "ru_y    1411\n",
      "ld_x     490\n",
      "ld_y    1511\n",
      "rd_x     879\n",
      "rd_y    1622\n",
      "Name: 4444, dtype: object\n",
      "IMG_4450.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D42F98>\n",
      "(416, 416, 3)\n",
      "Found 4 boxes for img\n",
      "car 0.89 (0, 26) (489, 463)\n",
      "car 0.91 (959, 429) (3023, 2222)\n",
      "car 0.94 (768, 103) (1530, 555)\n",
      "car 0.95 (286, 33) (1006, 507)\n",
      "1.9681387689997791\n",
      "lu_x     455\n",
      "lu_y    1080\n",
      "ru_x     838\n",
      "ru_y    1204\n",
      "ld_x     442\n",
      "ld_y    1269\n",
      "rd_x     803\n",
      "rd_y    1394\n",
      "Name: 4450, dtype: object\n",
      "IMG_4492.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129AFD240>\n",
      "(416, 416, 3)\n",
      "Found 4 boxes for img\n",
      "truck 0.80 (1117, 324) (2395, 1275)\n",
      "car 0.37 (1117, 324) (2395, 1275)\n",
      "car 0.90 (2260, 538) (3023, 1588)\n",
      "car 0.95 (0, 278) (1567, 1004)\n",
      "1.7787031990010291\n",
      "lu_x    235\n",
      "lu_y    575\n",
      "ru_x    389\n",
      "ru_y    609\n",
      "ld_x    225\n",
      "ld_y    664\n",
      "rd_x    376\n",
      "rd_y    695\n",
      "Name: 4492, dtype: object\n",
      "IMG_4486.JPG\n",
      "<PIL.Image.Image image mode=RGB size=3024x4032 at 0x129D2BCC0>\n",
      "(416, 416, 3)\n",
      "Found 2 boxes for img\n",
      "car 0.99 (35, 332) (1293, 1057)\n",
      "car 1.00 (705, 263) (3014, 2086)\n",
      "1.9249791759975778\n",
      "lu_x     140\n",
      "lu_y     956\n",
      "ru_x     378\n",
      "ru_y    1041\n",
      "ld_x     142\n",
      "ld_y    1129\n",
      "rd_x     374\n",
      "rd_y    1220\n",
      "Name: 4486, dtype: object\n"
     ]
    }
   ],
   "source": [
    "columns = [\"lu_x\",\"lu_y\",\"ru_x\",\"ru_y\",\"ld_x\",\"ld_y\",\"rd_x\",\"rd_y\"]\n",
    "df_concat = pd.Series([0,0,0,0,0,0,0,0],index = columns)\n",
    "df_resize_concat = pd.Series([0,0,0,0,0,0,0,0],index = columns)\n",
    "date = 20190125\n",
    "excel_df = pd.read_excel(f'./{date}/{date}list.xlsx',names=columns, index=0)\n",
    "\n",
    "df_na = (excel_df.iloc[1:,:]).dropna()\n",
    "df = df_na[1:]\n",
    "img_path = f\"/Users/naka345/Desktop/deeplearning/number_plate/{date}/{date}img\"\n",
    "output_path = \"/Users/naka345/Desktop/deeplearning/number_plate/output/car/\"\n",
    "output_csv_path = \"/Users/naka345/Desktop/deeplearning/number_plate/output/csv/\"\n",
    "train_csv_path = \"/Users/naka345/Desktop/deeplearning/number_plate/output/train/\"\n",
    "ls = glob.glob(img_path + \"/*.JPG\")\n",
    "c=0\n",
    "for path in ls:\n",
    "    file_name = path.split('/')[-1]\n",
    "    file_num = re.sub(r'\\D', '', file_name)\n",
    "    vertex = df.loc[int(file_num)]\n",
    "    print(file_name)\n",
    "    image = Image.open(path)\n",
    "    image = image.rotate(270, expand=True)\n",
    "    image_size = image.size\n",
    "    org_image = image.copy()\n",
    "    image, out_boxes, out_scores, out_classes = myYolo.detect_image(image)\n",
    "    image.save(output_path + '../' + file_name)\n",
    "    predict_pos = choice_box(vertex, out_boxes, out_scores, out_classes, image_size)\n",
    "    if predict_pos is None:\n",
    "        del image,org_image,vertex\n",
    "        continue\n",
    "    plate_npx=np.array([vertex[\"lu_x\"],vertex[\"ld_x\"],vertex[\"ru_x\"],vertex[\"rd_x\"]])\n",
    "    plate_npy=np.array([vertex[\"lu_y\"],vertex[\"ld_y\"],vertex[\"ru_y\"],vertex[\"rd_y\"]])\n",
    "    # one car\n",
    "    one_car_img = org_image.crop((predict_pos['left'], predict_pos['top'], predict_pos['right'], predict_pos['bottom']))\n",
    "    one_car_img.save(output_path + file_name)\n",
    "    moved_vertex = number_plate_crop(one_car_img, vertex, predict_pos, file_name)\n",
    "\n",
    "    df_concat = pd.concat([df_concat, moved_vertex],axis=1)\n",
    "    # detect_char_on_plate()\n",
    "    resized_df = resize_image(one_car_img,file_num,moved_vertex)\n",
    "    df_resize_concat = pd.concat([df_resize_concat, resized_df],axis=1)\n",
    "    del image,org_image,vertex\n",
    "\n",
    "\n",
    "df_T=df_concat.T\n",
    "df_T[1:].to_csv(f'{output_csv_path}{date}.csv')\n",
    "df_concat_T=df_resize_concat.T\n",
    "df_concat_T[1:].to_csv(f'{train_csv_path}{date}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choice_box(vertex, out_boxes, out_scores, out_classes, image_size):\n",
    "    predict_pos=[]\n",
    "    x=[vertex[\"lu_x\"],vertex[\"ld_x\"],vertex[\"ru_x\"],vertex[\"rd_x\"]]\n",
    "    y=[vertex[\"lu_y\"],vertex[\"ld_y\"],vertex[\"ru_y\"],vertex[\"rd_y\"]]\n",
    "\n",
    "    for (out_box, out_score, out_class) in zip(out_boxes,out_scores,out_classes):\n",
    "        pos_dict = created_boxes_vertex_dict(out_box, image_size)\n",
    "        x_pos = xpos_in_box(pos_dict, x)\n",
    "        y_pos = ypos_in_box(pos_dict, y)\n",
    "\n",
    "        if x_pos and y_pos:\n",
    "            pos_dict.update({\"score\":out_score})\n",
    "            predict_pos.append(pos_dict)\n",
    "    if len(predict_pos) >= 2:\n",
    "        max_score=0\n",
    "        for target_box in predict_pos:\n",
    "            if target_box[\"score\"]>max_score:\n",
    "                max_score = target_box[\"score\"]\n",
    "                return_box = target_box\n",
    "            else:\n",
    "                return_box = None\n",
    "        return return_box\n",
    "    elif len(predict_pos) == 0:\n",
    "        return None\n",
    "    return predict_pos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def created_boxes_vertex_dict(out_box, size):\n",
    "    top, left, bottom, right = out_box\n",
    "    top = max(0, np.floor(top + 0.5).astype('int32'))\n",
    "    left = max(0, np.floor(left + 0.5).astype('int32'))\n",
    "    bottom = min(size[1], np.floor(bottom + 0.5).astype('int32'))\n",
    "    right = min(size[0], np.floor(right + 0.5).astype('int32'))\n",
    "    box = [top, left, bottom, right]\n",
    "    \n",
    "    pos_dict={}\n",
    "    pos_name = [\"top\", \"left\", \"bottom\", \"right\"]\n",
    "    for pos,name in zip(box, pos_name):\n",
    "        pos_dict[name] = pos\n",
    "    return pos_dict\n",
    "\n",
    "def xpos_in_box(pos_dict, arr):\n",
    "    for x_pos in arr:\n",
    "        if (pos_dict[\"left\"] < x_pos < pos_dict[\"right\"]) == False:\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "def ypos_in_box(pos_dict, arr):\n",
    "    for y_pos in arr:\n",
    "        if (pos_dict[\"top\"] < y_pos < pos_dict[\"bottom\"]) == False:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   \\n        print(out_boxes.shape)\\n        out_boxes=np.append(out_boxes,np.array([[1597, 1299, 1757, 1542]]),axis=0)\\n        out_scores=np.append(out_scores,np.array([0.5]),axis=0)\\n        out_classes=np.append(out_classes,np.array([2]),axis=0)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''   \n",
    "        print(out_boxes.shape)\n",
    "        out_boxes=np.append(out_boxes,np.array([[1597, 1299, 1757, 1542]]),axis=0)\n",
    "        out_scores=np.append(out_scores,np.array([0.5]),axis=0)\n",
    "        out_classes=np.append(out_classes,np.array([2]),axis=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_and_update(vertex, corner_name, position):\n",
    "    move_pos = vertex[corner_name] - position\n",
    "    vertex.at[corner_name] = move_pos\n",
    "    return move_pos, vertex\n",
    "    \n",
    "def number_plate_crop(image, vertex, predict_pos, file_name):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    tmp_image = np.asarray(image)\n",
    "    ocv_image = tmp_image[:, :, ::-1].copy()\n",
    "    \n",
    "    move_lux, vertex = move_and_update(vertex, \"lu_x\", predict_pos[\"left\"])\n",
    "    move_ldx, vertex = move_and_update(vertex, \"ld_x\", predict_pos[\"left\"])\n",
    "    move_rux, vertex = move_and_update(vertex, \"ru_x\", predict_pos[\"left\"])\n",
    "    move_rdx, vertex = move_and_update(vertex, \"rd_x\", predict_pos[\"left\"])\n",
    "    move_luy, vertex = move_and_update(vertex, \"lu_y\", predict_pos[\"top\"])\n",
    "    move_ldy, vertex = move_and_update(vertex, \"ld_y\", predict_pos[\"top\"])\n",
    "    move_ruy, vertex = move_and_update(vertex, \"ru_y\", predict_pos[\"top\"])\n",
    "    move_rdy, vertex = move_and_update(vertex, \"rd_y\", predict_pos[\"top\"])\n",
    "    \n",
    "    pts1 = np.float32([[move_lux,move_luy],[move_rux,move_ruy],[move_ldx,move_ldy],[move_rdx,move_rdy]])\n",
    "    pts2 = np.float32([[0,0],[640,0],[0,320],[640,320]])\n",
    "    #透視変換\n",
    "    M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "    rst = cv2.warpPerspective(ocv_image,M,(640,320))\n",
    "    cv2.imwrite('/Users/naka345/Desktop/deeplearning/number_plate/output/number/'+file_name,rst)\n",
    "    \n",
    "    return vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lu_x</th>\n",
       "      <th>lu_y</th>\n",
       "      <th>ru_x</th>\n",
       "      <th>ru_y</th>\n",
       "      <th>ld_x</th>\n",
       "      <th>ld_y</th>\n",
       "      <th>rd_x</th>\n",
       "      <th>rd_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4422</th>\n",
       "      <td>1316</td>\n",
       "      <td>1517</td>\n",
       "      <td>1556</td>\n",
       "      <td>1581</td>\n",
       "      <td>1296</td>\n",
       "      <td>1649</td>\n",
       "      <td>1539</td>\n",
       "      <td>1718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4423</th>\n",
       "      <td>325</td>\n",
       "      <td>1027</td>\n",
       "      <td>586</td>\n",
       "      <td>1125</td>\n",
       "      <td>319</td>\n",
       "      <td>1192</td>\n",
       "      <td>578</td>\n",
       "      <td>1296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4424</th>\n",
       "      <td>1192</td>\n",
       "      <td>1578</td>\n",
       "      <td>1452</td>\n",
       "      <td>1652</td>\n",
       "      <td>1180</td>\n",
       "      <td>1726</td>\n",
       "      <td>1432</td>\n",
       "      <td>1808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4427</th>\n",
       "      <td>1296</td>\n",
       "      <td>1514</td>\n",
       "      <td>1592</td>\n",
       "      <td>1608</td>\n",
       "      <td>1270</td>\n",
       "      <td>1672</td>\n",
       "      <td>1560</td>\n",
       "      <td>1768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430</th>\n",
       "      <td>1186</td>\n",
       "      <td>1658</td>\n",
       "      <td>1478</td>\n",
       "      <td>1760</td>\n",
       "      <td>1174</td>\n",
       "      <td>1814</td>\n",
       "      <td>1456</td>\n",
       "      <td>1920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4431</th>\n",
       "      <td>1047</td>\n",
       "      <td>1671</td>\n",
       "      <td>1347</td>\n",
       "      <td>1800</td>\n",
       "      <td>1026</td>\n",
       "      <td>1866</td>\n",
       "      <td>1332</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4432</th>\n",
       "      <td>1116</td>\n",
       "      <td>1428</td>\n",
       "      <td>1450</td>\n",
       "      <td>1530</td>\n",
       "      <td>1092</td>\n",
       "      <td>1618</td>\n",
       "      <td>1418</td>\n",
       "      <td>1724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4433</th>\n",
       "      <td>1294</td>\n",
       "      <td>1680</td>\n",
       "      <td>1684</td>\n",
       "      <td>1844</td>\n",
       "      <td>1274</td>\n",
       "      <td>1864</td>\n",
       "      <td>1644</td>\n",
       "      <td>2040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4435</th>\n",
       "      <td>1490</td>\n",
       "      <td>1812</td>\n",
       "      <td>1966</td>\n",
       "      <td>2010</td>\n",
       "      <td>1446</td>\n",
       "      <td>2034</td>\n",
       "      <td>1908</td>\n",
       "      <td>2244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4436</th>\n",
       "      <td>691</td>\n",
       "      <td>1704</td>\n",
       "      <td>1163</td>\n",
       "      <td>1860</td>\n",
       "      <td>663</td>\n",
       "      <td>1922</td>\n",
       "      <td>1119</td>\n",
       "      <td>2084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4437</th>\n",
       "      <td>294</td>\n",
       "      <td>1178</td>\n",
       "      <td>604</td>\n",
       "      <td>1302</td>\n",
       "      <td>276</td>\n",
       "      <td>1354</td>\n",
       "      <td>570</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4438</th>\n",
       "      <td>457</td>\n",
       "      <td>1222</td>\n",
       "      <td>829</td>\n",
       "      <td>1342</td>\n",
       "      <td>431</td>\n",
       "      <td>1412</td>\n",
       "      <td>789</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4439</th>\n",
       "      <td>607</td>\n",
       "      <td>1406</td>\n",
       "      <td>1067</td>\n",
       "      <td>1552</td>\n",
       "      <td>581</td>\n",
       "      <td>1630</td>\n",
       "      <td>1039</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4441</th>\n",
       "      <td>992</td>\n",
       "      <td>1118</td>\n",
       "      <td>1280</td>\n",
       "      <td>1214</td>\n",
       "      <td>972</td>\n",
       "      <td>1290</td>\n",
       "      <td>1256</td>\n",
       "      <td>1392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4442</th>\n",
       "      <td>1232</td>\n",
       "      <td>1467</td>\n",
       "      <td>1553</td>\n",
       "      <td>1580</td>\n",
       "      <td>1208</td>\n",
       "      <td>1641</td>\n",
       "      <td>1520</td>\n",
       "      <td>1758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4443</th>\n",
       "      <td>920</td>\n",
       "      <td>1286</td>\n",
       "      <td>1202</td>\n",
       "      <td>1385</td>\n",
       "      <td>897</td>\n",
       "      <td>1449</td>\n",
       "      <td>1176</td>\n",
       "      <td>1548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4444</th>\n",
       "      <td>1326</td>\n",
       "      <td>1562</td>\n",
       "      <td>1721</td>\n",
       "      <td>1666</td>\n",
       "      <td>1301</td>\n",
       "      <td>1766</td>\n",
       "      <td>1690</td>\n",
       "      <td>1877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4447</th>\n",
       "      <td>1226</td>\n",
       "      <td>1278</td>\n",
       "      <td>1568</td>\n",
       "      <td>1400</td>\n",
       "      <td>1196</td>\n",
       "      <td>1460</td>\n",
       "      <td>1534</td>\n",
       "      <td>1590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4449</th>\n",
       "      <td>337</td>\n",
       "      <td>1044</td>\n",
       "      <td>643</td>\n",
       "      <td>1134</td>\n",
       "      <td>331</td>\n",
       "      <td>1221</td>\n",
       "      <td>628</td>\n",
       "      <td>1317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4450</th>\n",
       "      <td>1414</td>\n",
       "      <td>1509</td>\n",
       "      <td>1797</td>\n",
       "      <td>1633</td>\n",
       "      <td>1401</td>\n",
       "      <td>1698</td>\n",
       "      <td>1762</td>\n",
       "      <td>1823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4452</th>\n",
       "      <td>1035</td>\n",
       "      <td>1056</td>\n",
       "      <td>1263</td>\n",
       "      <td>1124</td>\n",
       "      <td>1031</td>\n",
       "      <td>1200</td>\n",
       "      <td>1257</td>\n",
       "      <td>1271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4453</th>\n",
       "      <td>915</td>\n",
       "      <td>879</td>\n",
       "      <td>1169</td>\n",
       "      <td>965</td>\n",
       "      <td>899</td>\n",
       "      <td>1052</td>\n",
       "      <td>1139</td>\n",
       "      <td>1142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4459</th>\n",
       "      <td>1361</td>\n",
       "      <td>1722</td>\n",
       "      <td>1827</td>\n",
       "      <td>1896</td>\n",
       "      <td>1319</td>\n",
       "      <td>1947</td>\n",
       "      <td>1775</td>\n",
       "      <td>2127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4462</th>\n",
       "      <td>400</td>\n",
       "      <td>1226</td>\n",
       "      <td>778</td>\n",
       "      <td>1361</td>\n",
       "      <td>382</td>\n",
       "      <td>1436</td>\n",
       "      <td>748</td>\n",
       "      <td>1576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4468</th>\n",
       "      <td>1935</td>\n",
       "      <td>1895</td>\n",
       "      <td>2447</td>\n",
       "      <td>2073</td>\n",
       "      <td>1877</td>\n",
       "      <td>2108</td>\n",
       "      <td>2358</td>\n",
       "      <td>2301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4469</th>\n",
       "      <td>1426</td>\n",
       "      <td>1584</td>\n",
       "      <td>1904</td>\n",
       "      <td>1760</td>\n",
       "      <td>1396</td>\n",
       "      <td>1820</td>\n",
       "      <td>1856</td>\n",
       "      <td>1990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4471</th>\n",
       "      <td>538</td>\n",
       "      <td>1376</td>\n",
       "      <td>997</td>\n",
       "      <td>1565</td>\n",
       "      <td>491</td>\n",
       "      <td>1598</td>\n",
       "      <td>931</td>\n",
       "      <td>1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4474</th>\n",
       "      <td>720</td>\n",
       "      <td>1585</td>\n",
       "      <td>1323</td>\n",
       "      <td>1806</td>\n",
       "      <td>672</td>\n",
       "      <td>1825</td>\n",
       "      <td>1242</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4486</th>\n",
       "      <td>845</td>\n",
       "      <td>1219</td>\n",
       "      <td>1083</td>\n",
       "      <td>1304</td>\n",
       "      <td>847</td>\n",
       "      <td>1392</td>\n",
       "      <td>1079</td>\n",
       "      <td>1483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>1137</td>\n",
       "      <td>784</td>\n",
       "      <td>1307</td>\n",
       "      <td>814</td>\n",
       "      <td>1129</td>\n",
       "      <td>874</td>\n",
       "      <td>1298</td>\n",
       "      <td>908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4492</th>\n",
       "      <td>1352</td>\n",
       "      <td>899</td>\n",
       "      <td>1506</td>\n",
       "      <td>933</td>\n",
       "      <td>1342</td>\n",
       "      <td>988</td>\n",
       "      <td>1493</td>\n",
       "      <td>1019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4495</th>\n",
       "      <td>1392</td>\n",
       "      <td>1020</td>\n",
       "      <td>1569</td>\n",
       "      <td>1058</td>\n",
       "      <td>1385</td>\n",
       "      <td>1124</td>\n",
       "      <td>1556</td>\n",
       "      <td>1160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4496</th>\n",
       "      <td>1422</td>\n",
       "      <td>646</td>\n",
       "      <td>1561</td>\n",
       "      <td>674</td>\n",
       "      <td>1397</td>\n",
       "      <td>737</td>\n",
       "      <td>1540</td>\n",
       "      <td>764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4507</th>\n",
       "      <td>308</td>\n",
       "      <td>682</td>\n",
       "      <td>531</td>\n",
       "      <td>739</td>\n",
       "      <td>293</td>\n",
       "      <td>804</td>\n",
       "      <td>516</td>\n",
       "      <td>861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4508</th>\n",
       "      <td>974</td>\n",
       "      <td>966</td>\n",
       "      <td>1161</td>\n",
       "      <td>1008</td>\n",
       "      <td>975</td>\n",
       "      <td>1074</td>\n",
       "      <td>1155</td>\n",
       "      <td>1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4509</th>\n",
       "      <td>1380</td>\n",
       "      <td>1077</td>\n",
       "      <td>1590</td>\n",
       "      <td>1128</td>\n",
       "      <td>1368</td>\n",
       "      <td>1185</td>\n",
       "      <td>1574</td>\n",
       "      <td>1238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4510</th>\n",
       "      <td>376</td>\n",
       "      <td>737</td>\n",
       "      <td>628</td>\n",
       "      <td>801</td>\n",
       "      <td>359</td>\n",
       "      <td>869</td>\n",
       "      <td>602</td>\n",
       "      <td>935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4513</th>\n",
       "      <td>85</td>\n",
       "      <td>747</td>\n",
       "      <td>322</td>\n",
       "      <td>795</td>\n",
       "      <td>56</td>\n",
       "      <td>876</td>\n",
       "      <td>289</td>\n",
       "      <td>928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4516</th>\n",
       "      <td>342</td>\n",
       "      <td>492</td>\n",
       "      <td>574</td>\n",
       "      <td>534</td>\n",
       "      <td>312</td>\n",
       "      <td>618</td>\n",
       "      <td>546</td>\n",
       "      <td>669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lu_x  lu_y  ru_x  ru_y  ld_x  ld_y  rd_x  rd_y\n",
       "4422  1316  1517  1556  1581  1296  1649  1539  1718\n",
       "4423   325  1027   586  1125   319  1192   578  1296\n",
       "4424  1192  1578  1452  1652  1180  1726  1432  1808\n",
       "4427  1296  1514  1592  1608  1270  1672  1560  1768\n",
       "4430  1186  1658  1478  1760  1174  1814  1456  1920\n",
       "4431  1047  1671  1347  1800  1026  1866  1332  1995\n",
       "4432  1116  1428  1450  1530  1092  1618  1418  1724\n",
       "4433  1294  1680  1684  1844  1274  1864  1644  2040\n",
       "4435  1490  1812  1966  2010  1446  2034  1908  2244\n",
       "4436   691  1704  1163  1860   663  1922  1119  2084\n",
       "4437   294  1178   604  1302   276  1354   570  1480\n",
       "4438   457  1222   829  1342   431  1412   789  1536\n",
       "4439   607  1406  1067  1552   581  1630  1039  1790\n",
       "4441   992  1118  1280  1214   972  1290  1256  1392\n",
       "4442  1232  1467  1553  1580  1208  1641  1520  1758\n",
       "4443   920  1286  1202  1385   897  1449  1176  1548\n",
       "4444  1326  1562  1721  1666  1301  1766  1690  1877\n",
       "4447  1226  1278  1568  1400  1196  1460  1534  1590\n",
       "4449   337  1044   643  1134   331  1221   628  1317\n",
       "4450  1414  1509  1797  1633  1401  1698  1762  1823\n",
       "4452  1035  1056  1263  1124  1031  1200  1257  1271\n",
       "4453   915   879  1169   965   899  1052  1139  1142\n",
       "4459  1361  1722  1827  1896  1319  1947  1775  2127\n",
       "4462   400  1226   778  1361   382  1436   748  1576\n",
       "4468  1935  1895  2447  2073  1877  2108  2358  2301\n",
       "4469  1426  1584  1904  1760  1396  1820  1856  1990\n",
       "4471   538  1376   997  1565   491  1598   931  1794\n",
       "4474   720  1585  1323  1806   672  1825  1242  2050\n",
       "4486   845  1219  1083  1304   847  1392  1079  1483\n",
       "4490  1137   784  1307   814  1129   874  1298   908\n",
       "4492  1352   899  1506   933  1342   988  1493  1019\n",
       "4495  1392  1020  1569  1058  1385  1124  1556  1160\n",
       "4496  1422   646  1561   674  1397   737  1540   764\n",
       "4507   308   682   531   739   293   804   516   861\n",
       "4508   974   966  1161  1008   975  1074  1155  1122\n",
       "4509  1380  1077  1590  1128  1368  1185  1574  1238\n",
       "4510   376   737   628   801   359   869   602   935\n",
       "4513    85   747   322   795    56   876   289   928\n",
       "4516   342   492   574   534   312   618   546   669"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(read_image,file_num, csv_df):\n",
    "    print(csv_df)\n",
    "    img_height, img_width = read_image.size\n",
    "\n",
    "\n",
    "    width = 200\n",
    "    height = 200\n",
    "    size = (height, width)\n",
    "    resize_image=read_image.resize(size, Image.LANCZOS)\n",
    "\n",
    "    resized_df = csv_df.copy()\n",
    "\n",
    "    zip_ratio_w = img_width/width\n",
    "    zip_ratio_h = img_height/height\n",
    "\n",
    "    resized_df.iloc[::2] /= zip_ratio_w\n",
    "    resized_df.iloc[1::2] /= zip_ratio_h\n",
    "\n",
    "    resize_image.save(f\"./output/train/train_{file_num}.JPG\")\n",
    "\n",
    "    return resized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
